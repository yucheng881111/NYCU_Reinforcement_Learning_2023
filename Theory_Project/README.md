# Abstract
Modern Deep Reinforcement Learning (RL) algorithms require estimates of the maximal Q-value, which are difficult to compute in continuous domains with an infinite number of possible actions. This work introduce a new update rule for online and offline RL which directly models the maximal value using Extreme Value Theory. The key insight is to introduce an objective that directly estimates the optimal soft-value functions (LogSumExp) in the maximum entropy RL setting without needing to sample from a policy. 

Video link (caption generated by Youtube automatically): https://youtu.be/dwSzPk2WKi0

All the materials: https://drive.google.com/drive/folders/1PiphGZXt8nar3LKmDHz_TZdaSZCIOcFz?usp=share_link
